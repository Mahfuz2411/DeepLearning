### ðŸ”¹ What is the Sigmoid Function? 

The Sigmoid function is a type of activation function used in neural networks. It maps any real-valued number to a value between 0 and 1, making it useful when you want to interpret the output as a probability.

### ðŸ”¹ Formula

<!-- ![sigmoid activation function](../../assets/sigmoid01.png)  -->

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

<!-- Sigmoid: Ïƒ(x) = 1 / (1 + e^(-x)) -->


Where:
- x is the input to the function
- e is Eulerâ€™s number (approx 2.718)


### ðŸ”¹ Graph

The Sigmoid function has an "S"-shaped curve. It smoothly transitions from 0 to 1 as the input \( x \) goes from negative to positive infinity.

- When \( x \to -\infty \), \( \sigma(x) \to 0 \)
- When \( x = 0 \), \( \sigma(x) = 0.5 \)
- When \( x \to +\infty \), \( \sigma(x) \to 1 \)

This shape helps squash large input values into a bounded range, which is useful for modeling probabilities.

The curve is centered at \( x = 0 \) and is symmetric around this point.


### ðŸ”¹ Example Outputs

| Input (x) | Output (Ïƒ(x)) |
|-----------|---------------|
| -3        | â‰ˆ 0.047       |
| -1        | â‰ˆ 0.269       |
|  0        | 0.5           |
|  1        | â‰ˆ 0.731       |
|  3        | â‰ˆ 0.953       |

### ðŸ”¹ Why use Sigmoid?

- Smooth curve 
- Output between 0 and 1 â†’ good for binary classification 
- Differentiable â†’ good for backpropagation 

### ðŸ”¹ Drawbacks
- Vanishing Gradient Problem: For very large or very small values of x, the gradient becomes close to zero. This slows learning.
- Not zero-centered: This can cause slow convergence in gradient-based optimization.

### ðŸ”¹ Derivative (Used in Backpropagation)
<!-- ![derivation of sigmoid](../../assets/sigmoid02.png) -->

$$
\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))
$$

### ðŸ”¹ When to Use
Use sigmoid mainly in the output layer for binary classification problems (like spam detection: spam or not).